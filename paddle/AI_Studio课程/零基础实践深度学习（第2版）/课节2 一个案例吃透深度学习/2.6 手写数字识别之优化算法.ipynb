{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2.6 手写数字识别之优化算法\n",
    "\n",
    "**第2.5节**我们明确了分类任务的损失函数（优化目标）的相关概念和实现方法，本节我们依旧横向展开\"横纵式\"教学法，如 **图1** 所示，本节主要探讨在手写数字识别任务中，使得损失达到最小的参数取值的实现方法。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/81405034cc7c4315b33d2aae990edc8013626c7dcdd54d56bc9afce9127c9be0\" width=\"1000\" hegiht=\"\" ></center>\n",
    "<center>图1：“横纵式”教学法 — 优化算法</center>\n",
    "<br></br>\n",
    "\n",
    "**前提条件**\n",
    "\n",
    "在优化算法之前，需要进行数据处理、设计神经网络结构，代码与上一节保持一致，此处直接调用封装好的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "from data_process import get_MNIST_dataloader\n",
    "from MNIST_network import MNIST\n",
    "\n",
    "train_loader, test_loader = get_MNIST_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.6.1 学习率\n",
    "\n",
    "在深度学习神经网络模型中，通常使用标准的随机梯度下降算法更新参数，学习率代表参数更新幅度的大小，即步长。当学习率最优时，模型的有效容量最大，最终能达到的效果最好。学习率和深度学习任务类型有关，合适的学习率往往需要大量的实验和调参经验。探索学习率最优值时需要注意如下两点：\n",
    "\n",
    "- **学习率不是越小越好**。学习率越小，损失函数的变化速度越慢，意味着我们需要花费更长的时间进行收敛，如 **图2** 左图所示。\n",
    "- **学习率不是越大越好**。只根据总样本集中的一个批次计算梯度，抽样误差会导致计算出的梯度不是全局最优的方向，且存在波动。在接近最优解时，过大的学习率会导致参数在最优解附近震荡，损失难以收敛，如 **图2** 右图所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/1e0f066dc9fa4e2bbc942447bdc0578c2ffc6afc15684154ae84bcf31b298d7b\" width=\"500\" hegiht=\"\" ></center>\n",
    "<center><br>图2: 不同学习率（步长过大/过小）的示意图</br></center>\n",
    "<br></br>\n",
    "\n",
    "在训练前，我们往往不清楚一个特定问题设置成怎样的学习率是合理的，因此在训练时可以尝试调小或调大，通过观察Loss下降的情况判断合理的学习率，设置学习率的代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0902 13:59:43.727392    98 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0902 13:59:43.731014    98 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: 2.672663927078247\n",
      "epoch: 0, batch: 200, loss is: 1.5871143341064453\n",
      "epoch: 0, batch: 400, loss is: 0.8906817436218262\n",
      "epoch: 0, batch: 600, loss is: 0.8545184135437012\n",
      "epoch: 0, batch: 800, loss is: 0.5248807668685913\n",
      "epoch: 1, batch: 0, loss is: 0.4951104521751404\n",
      "epoch: 1, batch: 200, loss is: 0.3496471643447876\n",
      "epoch: 1, batch: 400, loss is: 0.49374574422836304\n",
      "epoch: 1, batch: 600, loss is: 0.5409931540489197\n",
      "epoch: 1, batch: 800, loss is: 0.45909279584884644\n",
      "epoch: 2, batch: 0, loss is: 0.31246766448020935\n",
      "epoch: 2, batch: 200, loss is: 0.3619728684425354\n",
      "epoch: 2, batch: 400, loss is: 0.172702357172966\n",
      "epoch: 2, batch: 600, loss is: 0.2503175139427185\n",
      "epoch: 2, batch: 800, loss is: 0.2594001591205597\n",
      "epoch: 3, batch: 0, loss is: 0.18350714445114136\n",
      "epoch: 3, batch: 200, loss is: 0.31369107961654663\n",
      "epoch: 3, batch: 400, loss is: 0.24545088410377502\n",
      "epoch: 3, batch: 600, loss is: 0.22522735595703125\n",
      "epoch: 3, batch: 800, loss is: 0.20448797941207886\n",
      "epoch: 4, batch: 0, loss is: 0.33328133821487427\n",
      "epoch: 4, batch: 200, loss is: 0.208677276968956\n",
      "epoch: 4, batch: 400, loss is: 0.2658303380012512\n",
      "epoch: 4, batch: 600, loss is: 0.1646091341972351\n",
      "epoch: 4, batch: 800, loss is: 0.26463451981544495\n",
      "epoch: 5, batch: 0, loss is: 0.36017316579818726\n",
      "epoch: 5, batch: 200, loss is: 0.0982092022895813\n",
      "epoch: 5, batch: 400, loss is: 0.10808548331260681\n",
      "epoch: 5, batch: 600, loss is: 0.22752246260643005\n",
      "epoch: 5, batch: 800, loss is: 0.2808645963668823\n",
      "epoch: 6, batch: 0, loss is: 0.1768714040517807\n",
      "epoch: 6, batch: 200, loss is: 0.2420758754014969\n",
      "epoch: 6, batch: 400, loss is: 0.21574094891548157\n",
      "epoch: 6, batch: 600, loss is: 0.22417373955249786\n",
      "epoch: 6, batch: 800, loss is: 0.2321982979774475\n",
      "epoch: 7, batch: 0, loss is: 0.19339290261268616\n",
      "epoch: 7, batch: 200, loss is: 0.14796173572540283\n",
      "epoch: 7, batch: 400, loss is: 0.13107317686080933\n",
      "epoch: 7, batch: 600, loss is: 0.1809786856174469\n",
      "epoch: 7, batch: 800, loss is: 0.08983369171619415\n",
      "epoch: 8, batch: 0, loss is: 0.15719389915466309\n",
      "epoch: 8, batch: 200, loss is: 0.13746298849582672\n",
      "epoch: 8, batch: 400, loss is: 0.16063809394836426\n",
      "epoch: 8, batch: 600, loss is: 0.1283281445503235\n",
      "epoch: 8, batch: 800, loss is: 0.11324868351221085\n",
      "epoch: 9, batch: 0, loss is: 0.10661625117063522\n",
      "epoch: 9, batch: 200, loss is: 0.11727969348430634\n",
      "epoch: 9, batch: 400, loss is: 0.17486193776130676\n",
      "epoch: 9, batch: 600, loss is: 0.11552000790834427\n",
      "epoch: 9, batch: 800, loss is: 0.256372332572937\n"
     ]
    }
   ],
   "source": [
    "import paddle.nn.functional as F\n",
    "\n",
    "#仅优化算法的设置有所差别\n",
    "def train(model):\n",
    "    model.train()\n",
    "    \n",
    "    #设置不同初始学习率\n",
    "    opt = paddle.optimizer.SGD(learning_rate=0.001, parameters=model.parameters())\n",
    "    # opt = paddle.optimizer.SGD(learning_rate=0.0001, parameters=model.parameters())\n",
    "    # opt = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())\n",
    "    \n",
    "    EPOCH_NUM = 10\n",
    "    loss_list = []\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            #准备数据\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "            \n",
    "            #前向计算的过程\n",
    "            predicts = model(images)\n",
    "            \n",
    "            #计算损失，取一个批次样本损失的平均值\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            \n",
    "            #每训练了100批次的数据，打印下当前Loss的情况\n",
    "            if batch_id % 200 == 0:\n",
    "                loss = avg_loss.numpy()[0]\n",
    "                loss_list.append(loss)\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}\".format(epoch_id, batch_id, loss))\n",
    "            \n",
    "            #后向传播，更新参数的过程\n",
    "            avg_loss.backward()\n",
    "            # 最小化loss,更新参数\n",
    "            opt.step()\n",
    "            # 清除梯度\n",
    "            opt.clear_grad()\n",
    "   \n",
    "    #保存模型参数\n",
    "    paddle.save(model.state_dict(), 'mnist.pdparams')\n",
    "    return loss_list\n",
    "    \n",
    "#创建模型    \n",
    "model = MNIST()\n",
    "#启动训练过程\n",
    "loss_list = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "绘制loss变化曲线："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tools import plot\n",
    "\n",
    "plot(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "##  2.6.2 学习率的四种主流优化算法\n",
    "\n",
    "学习率是优化器的一个参数，调整学习率看似是一件非常麻烦的事情，需要不断的调整步长，观察训练时间和Loss的变化。经过科研人员的不断的实验，当前已经形成了四种比较成熟的优化算法：SGD、Momentum、AdaGrad和Adam，效果如 **图3** 所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/f4cf80f95424411a85ad74998433317e721f56ddb4f64e6f8a28a27b6a1baa6b\" width=\"800\" hegiht=\"\" ></center>\n",
    "<center><br>图3: 不同学习率算法效果示意图</br></center>\n",
    "<br></br>\n",
    "\n",
    "* **[SGD：](https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-beta/api/paddle/optimizer/SGD_cn.html)** 随机梯度下降算法，每次训练少量数据，抽样偏差导致的参数收敛过程中震荡。\n",
    "\n",
    "* **[Momentum：](https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-beta/api/paddle/optimizer/Momentum_cn.html)** 引入物理“动量”的概念，累积速度，减少震荡，使参数更新的方向更稳定。\n",
    "\n",
    "每个批次的数据含有抽样误差，导致梯度更新的方向波动较大。如果我们引入物理动量的概念，给梯度下降的过程加入一定的“惯性”累积，就可以减少更新路径上的震荡，即每次更新的梯度由“历史多次梯度的累积方向”和“当次梯度”加权相加得到。历史多次梯度的累积方向往往是从全局视角更正确的方向，这与“惯性”的物理概念很像，也是为何其起名为“Momentum”的原因。类似不同品牌和材质的篮球有一定的重量差别，街头篮球队中的投手（擅长中远距离投篮）往往更喜欢稍重篮球。一个很重要的原因是，重的篮球惯性大，更不容易受到手势的小幅变形或风吹的影响。\n",
    "\n",
    "* **[AdaGrad：](https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-beta/api/paddle/optimizer/AdagradOptimizer_cn.html)** 根据不同参数距离最优解的远近，动态调整学习率。学习率逐渐下降，依据各参数变化大小调整学习率。\n",
    "\n",
    "通过调整学习率的实验可以发现：当某个参数的现值距离最优解较远时（表现为梯度的绝对值较大），我们期望参数更新的步长大一些，以便更快收敛到最优解。当某个参数的现值距离最优解较近时（表现为梯度的绝对值较小），我们期望参数的更新步长小一些，以便更精细的逼近最优解。类似于打高尔夫球，专业运动员第一杆开球时，通常会大力打一个远球，让球尽量落在洞口附近。当第二杆面对离洞口较近的球时，他会更轻柔而细致的推杆，避免将球打飞。与此类似，参数更新的步长应该随着优化过程逐渐减少，减少的程度与当前梯度的大小有关。根据这个思想编写的优化算法称为“AdaGrad”，Ada是Adaptive的缩写，表示“适应环境而变化”的意思。[RMSProp](https://www.paddlepaddle.org.cn/documentation/docs/zh/api_cn/optimizer_cn/RMSPropOptimizer_cn.html#rmspropoptimizer)是在AdaGrad基础上的改进，学习率随着梯度变化而适应，解决AdaGrad学习率急剧下降的问题。\n",
    "\n",
    "* **[Adam：](https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-beta/api/paddle/optimizer/Adam_cn.html)**  由于动量和自适应学习率两个优化思路是正交的，因此可以将两个思路结合起来，这是当前广泛应用的算法。\n",
    "\n",
    "\n",
    "\n",
    "------\n",
    "**说明：**\n",
    "\n",
    "每种优化算法均有更多的参数设置，详情可查阅[飞桨的官方API文档](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/index_cn.html)。理论最合理的未必在具体案例中最有效，所以模型调参是很有必要的，最优的模型配置往往是在一定“理论”和“经验”的指导下实验出来的。\n",
    "\n",
    "------\n",
    "\n",
    "我们可以尝试选择不同的优化算法训练模型，观察训练时间和损失变化的情况，代码实现如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [3.179514]\n",
      "epoch: 0, batch: 200, loss is: [0.41207898]\n",
      "epoch: 0, batch: 400, loss is: [0.14528263]\n",
      "epoch: 0, batch: 600, loss is: [0.17151298]\n",
      "epoch: 0, batch: 800, loss is: [0.20650986]\n",
      "epoch: 1, batch: 0, loss is: [0.048602]\n",
      "epoch: 1, batch: 200, loss is: [0.16017383]\n",
      "epoch: 1, batch: 400, loss is: [0.09366961]\n",
      "epoch: 1, batch: 600, loss is: [0.20587194]\n",
      "epoch: 1, batch: 800, loss is: [0.03139528]\n",
      "epoch: 2, batch: 0, loss is: [0.07924803]\n",
      "epoch: 2, batch: 200, loss is: [0.03837227]\n",
      "epoch: 2, batch: 400, loss is: [0.20707312]\n",
      "epoch: 2, batch: 600, loss is: [0.05167127]\n",
      "epoch: 2, batch: 800, loss is: [0.0629859]\n"
     ]
    }
   ],
   "source": [
    "#仅优化算法的设置有所差别\n",
    "def train(model):\n",
    "    model.train()\n",
    "    \n",
    "    #四种优化算法的设置方案，可以逐一尝试效果\n",
    "    opt = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())\n",
    "    # opt = paddle.optimizer.Momentum(learning_rate=0.01, momentum=0.9, parameters=model.parameters())\n",
    "    # opt = paddle.optimizer.Adagrad(learning_rate=0.01, parameters=model.parameters())\n",
    "    # opt = paddle.optimizer.Adam(learning_rate=0.01, parameters=model.parameters())\n",
    "    \n",
    "    EPOCH_NUM = 3\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            #准备数据\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "            \n",
    "            #前向计算的过程\n",
    "            predicts = model(images)\n",
    "            \n",
    "            #计算损失，取一个批次样本损失的平均值\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            \n",
    "            #每训练了100批次的数据，打印下当前Loss的情况\n",
    "            if batch_id % 200 == 0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}\".format(epoch_id, batch_id, avg_loss.numpy()))\n",
    "            \n",
    "            #后向传播，更新参数的过程\n",
    "            avg_loss.backward()\n",
    "            # 最小化loss,更新参数\n",
    "            opt.step()\n",
    "            # 清除梯度\n",
    "            opt.clear_grad()\n",
    "\n",
    "    #保存模型参数\n",
    "    paddle.save(model.state_dict(), 'mnist.pdparams')\n",
    "    \n",
    "#创建模型    \n",
    "model = MNIST()\n",
    "#启动训练过程\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.6.3 模型参数初始化\n",
    "\n",
    "模型参数初始化是指在训练前，给要训练的参数一个初始的值。比如我们最终的目的是走到山谷底，如果一开始把你放到半山腰和放到山脚，那是否能够顺利走到谷底以及走到谷底的速度是有很大差距的。同样，在我们训练神经网络的时候，训练参数初始值不同也会导致模型收敛速度和最终训练效果的不同。\n",
    "\n",
    "在PaddlePaddle框架中，MNIST模型中用到的`Conv2D`和`Linear`层都有`weight_attr`和`bias_attr`两个参数，默认为`None`，表示使用默认的权重参数属性。这两个参数可以使用模型的named_parameters()函数获得。\n",
    "\n",
    "通过下面的代码我们可以查看模型每层的参数情况：\n",
    "\n",
    "> 模型的state_dict函数可获取当前层及其子层的所有参数和可持久性buffers。并将所有参数和buffers存放在dict结构中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight Parameter containing:\n",
      "Tensor(shape=[20, 1, 5, 5], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [[[[ 0.00685916, -0.02827659, -0.00629800,  0.17840426,  0.07073421],\n",
      "          [ 0.15301149,  0.14512494, -0.08644208, -0.04089865,  0.06347622],\n",
      "          [-0.17223580,  0.53609502, -0.31562763,  0.31539580,  0.11763398],\n",
      "          [ 0.06057844,  0.12100003, -0.19036214, -0.32686502, -0.04253779],\n",
      "          [-0.24231870, -0.09162699, -0.00356645,  0.13624071,  0.35424659]]],\n",
      "\n",
      "\n",
      "        [[[ 0.19513041, -0.18949197, -0.06306379, -0.33953783, -0.12144631],\n",
      "          [-0.52912796, -0.04904719,  0.22196575, -0.21367665, -0.14066246],\n",
      "          [-0.47838733,  0.19762798,  0.32846957, -0.06876396, -0.05648636],\n",
      "          [-0.49627945,  0.04398900, -0.02770269, -0.09062629, -0.25601473],\n",
      "          [ 0.22629717,  0.12104782, -0.21879482, -0.29909152,  0.15422553]]],\n",
      "\n",
      "\n",
      "        [[[-0.57517582,  0.07872081,  0.24480270, -0.10398132, -0.21764469],\n",
      "          [-0.15390907,  0.03888793,  0.31359494,  0.05783065,  0.09863662],\n",
      "          [-0.00036609,  0.05385838, -0.06690086,  0.12735263, -0.56700391],\n",
      "          [-0.39829355, -0.05769483, -0.05732801,  0.33780238,  0.53295583],\n",
      "          [-0.15284447, -0.32235619, -0.12371407, -0.02103593,  0.06312265]]],\n",
      "\n",
      "\n",
      "        [[[ 0.23172377,  0.32362208,  0.05168466,  0.50253624, -0.42746297],\n",
      "          [ 0.18174489, -0.13101643, -0.20413736,  0.17553115,  0.19926409],\n",
      "          [ 0.52452153, -0.21518429, -0.14862841, -0.26547405,  0.15221493],\n",
      "          [ 0.24805403,  0.24819772, -0.02215577,  0.04740265,  0.31030568],\n",
      "          [ 0.13231085,  0.28731391,  0.17576139,  0.13701966, -0.38116997]]],\n",
      "\n",
      "\n",
      "        [[[ 0.12775137,  0.03297117, -0.17570035,  0.18195666, -0.47419620],\n",
      "          [ 0.33481011, -0.00369794,  0.17601000,  0.09904220, -0.01528732],\n",
      "          [-0.27129415, -0.38718089,  0.54803246,  0.14578563,  0.23014370],\n",
      "          [ 0.22600482, -0.23167014,  0.17863478, -0.02830615, -0.00889434],\n",
      "          [-0.53870600,  0.09300359, -0.09527846, -0.37450442,  0.08891475]]],\n",
      "\n",
      "\n",
      "        [[[-0.63375968,  0.01352617, -0.40009904, -0.61710560,  0.42513108],\n",
      "          [-0.17352255, -0.15771994,  0.08855379,  0.08413874,  0.15258929],\n",
      "          [ 0.29868862,  0.07071455, -0.06578474,  0.07363825, -0.37690639],\n",
      "          [ 0.44038859,  0.05002983, -0.36027536, -0.08144029, -0.13867600],\n",
      "          [ 0.20790854, -0.23763619,  0.25140327, -0.20502961,  0.51529437]]],\n",
      "\n",
      "\n",
      "        [[[-0.21660955,  0.22310224, -0.69941777,  0.03165551, -0.17004766],\n",
      "          [-0.53551018, -0.00880008, -0.03999506, -0.01059055,  0.07984187],\n",
      "          [-0.24571578,  0.48378825,  0.23929246, -0.21449521, -0.02937231],\n",
      "          [-0.16947500,  0.06570072,  0.26996130, -0.00225378, -0.45664248],\n",
      "          [ 0.07661092, -0.02118895, -0.23899272, -0.46309194,  0.00255588]]],\n",
      "\n",
      "\n",
      "        [[[ 0.01068391,  0.18249844, -0.57028669, -0.02930197, -0.49979338],\n",
      "          [ 0.23231256,  0.00567782, -0.46816435, -0.05387615, -0.06768128],\n",
      "          [-0.57407957, -0.38469478,  0.14671670, -0.19064607,  0.16242966],\n",
      "          [ 0.05997788,  0.14379767, -0.43261734, -0.17703882,  0.15757072],\n",
      "          [ 0.13257961, -0.19664960, -0.09354297,  0.29407963, -0.09218543]]],\n",
      "\n",
      "\n",
      "        [[[-0.05021130,  0.17844844, -0.42766276, -0.07062170,  0.16383797],\n",
      "          [ 0.21554719, -0.24867499, -0.04475411, -0.39096946,  0.26923472],\n",
      "          [-0.38155058, -0.20673595,  0.26717725,  0.17506047, -0.13850974],\n",
      "          [ 0.24715245,  0.50960469, -0.26948437, -0.34616211,  0.73909211],\n",
      "          [-0.04646778,  0.39078084, -0.20954891,  0.18042360, -0.52201396]]],\n",
      "\n",
      "\n",
      "        [[[ 0.08273765, -0.09105711, -0.12958071, -0.29861581, -0.19989121],\n",
      "          [ 0.18611236, -0.67686284, -0.18584950, -0.06461700, -0.56305975],\n",
      "          [ 0.06481798, -0.14299046, -0.17562066, -0.22412276, -0.28058848],\n",
      "          [ 0.37638661, -0.01545838,  0.02834374, -0.44421050,  0.24978781],\n",
      "          [ 0.47179404,  0.16267806,  0.40233117,  0.03811143, -0.01092976]]],\n",
      "\n",
      "\n",
      "        [[[ 0.02333970,  0.22006829, -0.54471511, -0.18041711, -0.12503794],\n",
      "          [ 0.10296388,  0.66934699,  0.06700514, -0.20929156, -0.44396263],\n",
      "          [ 0.17440335, -0.11310478, -0.27263820, -0.03610590,  0.34677291],\n",
      "          [ 0.10419460, -0.10790036, -0.02151075, -0.10813204,  0.24426167],\n",
      "          [-0.68452549, -0.05186544,  0.42406222,  0.13648765,  0.30811521]]],\n",
      "\n",
      "\n",
      "        [[[ 0.09500170, -0.36259845, -0.01421565, -0.51392865,  0.03173811],\n",
      "          [ 0.30929992, -0.37484398, -0.67092603,  0.38448986,  0.20572491],\n",
      "          [-0.06885525, -0.08177355,  0.03418944,  0.09427575, -0.09787619],\n",
      "          [-0.19711410,  0.03484981, -0.18318318, -0.29350778, -0.12663999],\n",
      "          [ 0.28633794,  0.76515442, -0.18395521,  0.04850828, -0.42525050]]],\n",
      "\n",
      "\n",
      "        [[[-0.35616848,  0.21384183, -0.30207559, -0.24183397, -0.18685399],\n",
      "          [-0.10017283, -0.26885203, -0.00999357, -0.20930922, -0.30007312],\n",
      "          [ 0.04770488, -0.29103780,  0.41294131, -0.16971968,  0.12665810],\n",
      "          [-0.17194323,  0.01729790, -0.00445249, -0.09733340, -0.43409133],\n",
      "          [-0.20486255,  0.16031756, -0.08515637, -0.48493597, -0.27581355]]],\n",
      "\n",
      "\n",
      "        [[[-0.12680070,  0.45840842,  0.34614697,  0.01164691, -0.18574916],\n",
      "          [ 0.38707557, -0.22326174, -0.49643895, -0.20572264,  0.16056405],\n",
      "          [ 0.12451726, -0.24313389,  0.52626842, -0.06479061,  0.00620653],\n",
      "          [-0.50085860,  0.08915006, -0.18392627, -0.27886039, -0.44590169],\n",
      "          [-0.05520649, -0.00192035,  0.19382079,  0.30146438, -0.19075648]]],\n",
      "\n",
      "\n",
      "        [[[ 0.17200373,  0.36045110,  0.41303283, -0.05697197,  0.09817324],\n",
      "          [ 0.24160154,  0.20104299, -0.18491277,  0.30328605, -0.15703280],\n",
      "          [ 0.14609288, -0.35724214,  0.21076484, -0.39886394, -0.52864009],\n",
      "          [-0.22453904, -0.02393566,  0.22230516, -0.23590966,  0.08467520],\n",
      "          [ 0.10224124,  0.40317741, -0.02853043, -0.15255164, -0.10038172]]],\n",
      "\n",
      "\n",
      "        [[[ 0.25365898, -0.12408470,  0.03224292, -0.62302363, -0.00868563],\n",
      "          [-0.20751679, -0.27265713,  0.10152783, -0.07019686,  0.20044255],\n",
      "          [-0.01536619,  0.24629453, -0.50585532,  0.23426042,  0.34668776],\n",
      "          [ 0.37670568, -0.04776695,  0.13384424, -0.14209406, -0.57875556],\n",
      "          [-0.26221901, -0.30681297, -0.01673537,  0.17959179,  0.21828853]]],\n",
      "\n",
      "\n",
      "        [[[-0.16501173,  0.17561746,  0.07628912, -0.11273076,  0.13463901],\n",
      "          [ 0.11364105,  0.01849935, -0.31740916, -0.60119075,  0.04458283],\n",
      "          [-0.07207803,  0.12399440,  0.31883177, -0.04306261,  0.31809899],\n",
      "          [ 0.53678864,  0.46501875,  0.10381963, -0.00760704,  0.09934480],\n",
      "          [ 0.29437843, -0.22324999,  0.09065028, -0.09417098, -0.08439593]]],\n",
      "\n",
      "\n",
      "        [[[ 0.03022816,  0.12597114,  0.43907633,  0.34548521, -0.25947937],\n",
      "          [-0.45549130,  0.52134931, -0.32358417,  0.20811863, -0.34024143],\n",
      "          [-0.43153605, -0.25069806, -0.29741403, -0.22896214, -0.38898811],\n",
      "          [ 0.54590076, -0.12959005,  0.05260053, -0.17379124, -0.30745879],\n",
      "          [-0.37525591, -0.09658249, -0.17254332,  0.05943358, -0.03464231]]],\n",
      "\n",
      "\n",
      "        [[[ 0.20201388,  0.02919013, -0.08088612,  0.01559477,  0.17472650],\n",
      "          [ 0.02217738,  0.36532468,  0.09082818,  0.32516485,  0.62717056],\n",
      "          [-0.41841173, -0.20961802, -0.27907607, -0.53327650, -0.65651703],\n",
      "          [-0.03331180,  0.14785196,  0.18553858,  0.01868903, -0.59221572],\n",
      "          [ 0.06131181, -0.03520117, -0.85177898,  0.18555664,  0.01231959]]],\n",
      "\n",
      "\n",
      "        [[[-0.62316769,  0.08701386,  0.14838187, -0.23780005,  0.56156313],\n",
      "          [-0.15943550,  0.24376908,  0.34804633, -0.05670764, -0.17174038],\n",
      "          [-0.68133259, -0.22665225,  0.22788525,  0.81866211, -0.13602504],\n",
      "          [-0.25465700, -0.15599780, -0.28056905,  0.16332000,  0.44518515],\n",
      "          [-0.02113093, -0.30859464, -0.10784610,  0.37195674,  0.19204357]]]])\n",
      "conv1.bias Parameter containing:\n",
      "Tensor(shape=[20], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n",
      "conv2.weight Parameter containing:\n",
      "Tensor(shape=[20, 20, 5, 5], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [[[[-0.04842516, -0.08029010, -0.18321052,  0.13665083, -0.00310786],\n",
      "          [ 0.01316942,  0.01703477, -0.03289547, -0.06039572,  0.01729234],\n",
      "          [ 0.07795183, -0.01019255, -0.01619649, -0.03624577,  0.03045435],\n",
      "          [-0.05737413, -0.11153530, -0.13117975,  0.04525512, -0.04865235],\n",
      "          [ 0.01150290,  0.04230673,  0.04859869, -0.03920656, -0.06766543]],\n",
      "\n",
      "         [[ 0.03040275, -0.05579495, -0.03721107, -0.02039793,  0.02349767],\n",
      "          [ 0.10402218,  0.06842967,  0.04437708, -0.04695178,  0.03593487],\n",
      "          [ 0.07269627,  0.12320693, -0.03603446, -0.02725364,  0.14172958],\n",
      "          [ 0.07811373, -0.00567554,  0.03271879,  0.05423937, -0.03669309],\n",
      "          [-0.02546963,  0.13795105,  0.16941258,  0.01445939, -0.07495897]],\n",
      "\n",
      "         [[-0.00970204,  0.08717278,  0.03948886,  0.03161471, -0.01342452],\n",
      "          [-0.01307007, -0.26236951,  0.05505652,  0.01218966, -0.04867758],\n",
      "          [ 0.06803223, -0.06957099,  0.01415347, -0.02940745,  0.05541166],\n",
      "          [-0.01195714, -0.03342409,  0.07563422,  0.04806461,  0.05999428],\n",
      "          [ 0.03009684, -0.07667986, -0.13065594, -0.00281614, -0.03151121]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.05550532, -0.06291343,  0.10043516, -0.13087311,  0.03000470],\n",
      "          [-0.10445759, -0.10226472,  0.03296762, -0.14403141, -0.00440695],\n",
      "          [ 0.03932971,  0.04916470, -0.07688967, -0.00819338,  0.03226198],\n",
      "          [ 0.08393429,  0.03892666, -0.01030571, -0.01706931,  0.00926275],\n",
      "          [ 0.05829731, -0.17966139,  0.02374594, -0.00765490, -0.02068477]],\n",
      "\n",
      "         [[ 0.05165485,  0.12393295, -0.09693903, -0.06920276, -0.14272152],\n",
      "          [-0.14296801, -0.06419675, -0.08101837,  0.02699590, -0.01101026],\n",
      "          [ 0.07865785, -0.02847964, -0.04767105, -0.06292751, -0.02792644],\n",
      "          [ 0.03781227, -0.06337287,  0.10960077, -0.05567764,  0.00034367],\n",
      "          [-0.06738880, -0.02947779,  0.01456474, -0.02245717,  0.02129958]],\n",
      "\n",
      "         [[-0.02013259, -0.01687173,  0.01458535, -0.00427398, -0.07280238],\n",
      "          [-0.02416323, -0.03606015,  0.03001739,  0.00022196, -0.08827833],\n",
      "          [ 0.02064940,  0.17045812, -0.05209301,  0.03668752, -0.04805639],\n",
      "          [-0.10638451,  0.05070315, -0.07866702, -0.01839409,  0.03969995],\n",
      "          [ 0.00396901, -0.05066794,  0.02775213, -0.08091297,  0.05483221]]],\n",
      "\n",
      "\n",
      "        [[[ 0.04185633,  0.07163000,  0.02353612,  0.07301259,  0.01324197],\n",
      "          [-0.04896133,  0.03730998,  0.10001282, -0.01895486, -0.03815692],\n",
      "          [-0.04848228, -0.07031865, -0.08686718, -0.01998477, -0.05397718],\n",
      "          [-0.08679562,  0.07052211, -0.04362950, -0.00569952, -0.01177182],\n",
      "          [ 0.04117786, -0.02817946,  0.08220866,  0.11326317, -0.00927471]],\n",
      "\n",
      "         [[ 0.01212797,  0.04936966, -0.05634697,  0.05705793, -0.13667816],\n",
      "          [ 0.03572141,  0.00228636,  0.00175165,  0.04967417, -0.01179718],\n",
      "          [-0.06498671, -0.03038970, -0.02413954,  0.15372913, -0.04139369],\n",
      "          [ 0.01032100, -0.05294703,  0.00316197,  0.07240158,  0.00002777],\n",
      "          [ 0.10682158, -0.02229001, -0.00935003,  0.07681382,  0.00232255]],\n",
      "\n",
      "         [[ 0.00351084,  0.10372302, -0.04970520,  0.02191114, -0.11757401],\n",
      "          [-0.06228558, -0.08901671, -0.00931545, -0.03228965,  0.03276297],\n",
      "          [ 0.01107195,  0.01682849,  0.03364248,  0.07903329,  0.07512107],\n",
      "          [ 0.12478322,  0.00384532,  0.01956144, -0.10132410, -0.07275780],\n",
      "          [-0.02398505,  0.04186409,  0.04651655, -0.01202679, -0.00504838]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.04461821,  0.00808487,  0.00555061, -0.01431967, -0.02614857],\n",
      "          [ 0.04759842, -0.05613897, -0.00544772,  0.05292856,  0.00298492],\n",
      "          [-0.12115648, -0.02731785, -0.05948418,  0.04074679,  0.08466259],\n",
      "          [ 0.10574198, -0.04008232, -0.10394353, -0.02639852,  0.11049286],\n",
      "          [ 0.11733372,  0.05187315,  0.00404155, -0.04136150, -0.00336215]],\n",
      "\n",
      "         [[-0.04149555,  0.00675234,  0.01423796, -0.01474721,  0.00792756],\n",
      "          [-0.12927379, -0.00978363, -0.02188701,  0.07809865, -0.02926444],\n",
      "          [ 0.08665561, -0.00281964, -0.04393354,  0.01829349,  0.07911012],\n",
      "          [ 0.01448872,  0.05762992, -0.00321733, -0.03440291,  0.04463581],\n",
      "          [ 0.03841044, -0.10381646, -0.01932618, -0.03856980,  0.02094867]],\n",
      "\n",
      "         [[-0.02882951, -0.08565428, -0.09080452, -0.04000576, -0.07269020],\n",
      "          [ 0.12983690,  0.01571902,  0.07720028,  0.01375103,  0.03910543],\n",
      "          [-0.01024854,  0.01133631,  0.06438381,  0.07261378,  0.14840412],\n",
      "          [ 0.01374937, -0.04521683,  0.05477804, -0.01355262, -0.03515146],\n",
      "          [ 0.05718626, -0.16190849,  0.02147372, -0.00782114, -0.07004025]]],\n",
      "\n",
      "\n",
      "        [[[ 0.07583300, -0.02820885,  0.05300792,  0.00542720,  0.12516046],\n",
      "          [ 0.06020210,  0.11858690,  0.05682371, -0.00590254,  0.07037435],\n",
      "          [-0.10353146, -0.12717886, -0.06910776,  0.08883084, -0.00054811],\n",
      "          [ 0.01767521, -0.08097462,  0.00492033, -0.06323463, -0.00576807],\n",
      "          [ 0.02244565, -0.09164488,  0.06393596, -0.02257344, -0.01569874]],\n",
      "\n",
      "         [[-0.08124023,  0.15796214, -0.04945521,  0.01870681, -0.00390383],\n",
      "          [ 0.01539223, -0.02441236,  0.05842051,  0.00133073,  0.02581081],\n",
      "          [ 0.03391039,  0.02032443, -0.07897935,  0.02345095,  0.02789728],\n",
      "          [-0.04883847, -0.00576577, -0.02687601,  0.08461127,  0.01148212],\n",
      "          [ 0.00427470, -0.08370639, -0.07997342, -0.05005461,  0.02978116]],\n",
      "\n",
      "         [[-0.05750392,  0.07255925, -0.01636612, -0.03015611,  0.04846285],\n",
      "          [-0.00706771,  0.10853414,  0.04668500,  0.00376344,  0.04004541],\n",
      "          [ 0.04384706,  0.03126309,  0.05270825, -0.05617063,  0.11446072],\n",
      "          [ 0.01019831, -0.02750200, -0.07116103,  0.06956849,  0.16107149],\n",
      "          [ 0.01312344,  0.08240703,  0.06850206, -0.12185076,  0.09815940]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.01280733, -0.02550051, -0.03055892, -0.00413280, -0.13141689],\n",
      "          [-0.01717129,  0.01984228,  0.00275885, -0.03977481,  0.04500667],\n",
      "          [ 0.01178519, -0.04080101,  0.02944230,  0.03536454,  0.03784487],\n",
      "          [-0.00705960, -0.01413729, -0.00264809, -0.01100500, -0.00454286],\n",
      "          [-0.00722889, -0.05507685, -0.05607553, -0.00012039,  0.06392091]],\n",
      "\n",
      "         [[ 0.01779486, -0.07202761,  0.06065071,  0.00180589, -0.03458759],\n",
      "          [-0.03363177,  0.09722894, -0.11379460, -0.04269574,  0.03563803],\n",
      "          [ 0.06863476, -0.00360048,  0.05298167,  0.08426961,  0.08290426],\n",
      "          [-0.03084364, -0.01353161,  0.00667048,  0.05405263,  0.07051750],\n",
      "          [-0.00134665, -0.00061161, -0.03761236,  0.05313950,  0.03484831]],\n",
      "\n",
      "         [[ 0.13642707,  0.06661936, -0.13489626,  0.06779353, -0.03371165],\n",
      "          [ 0.06006323, -0.09677055, -0.13239473,  0.01005304,  0.02632247],\n",
      "          [ 0.05612664, -0.17812183, -0.04645997,  0.03781860,  0.04665733],\n",
      "          [-0.07935522, -0.02261722,  0.08028147,  0.05874356,  0.00949989],\n",
      "          [-0.04510846, -0.01954781,  0.08129469, -0.03879700, -0.04195885]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.04311617,  0.14076813,  0.03567293,  0.09146731, -0.05475181],\n",
      "          [-0.09110141, -0.07392356, -0.06702660, -0.12429566,  0.06178958],\n",
      "          [ 0.01095681, -0.09050345, -0.08802234, -0.06919430, -0.01185550],\n",
      "          [ 0.03364372, -0.04875413,  0.00528686, -0.01612952, -0.12905483],\n",
      "          [ 0.03738477, -0.09159284, -0.01696681,  0.02573332,  0.14408062]],\n",
      "\n",
      "         [[ 0.02396505,  0.01335742,  0.01688723,  0.11205415, -0.07941812],\n",
      "          [ 0.02427093, -0.05603881, -0.10590425,  0.00792881, -0.01656183],\n",
      "          [ 0.03530726,  0.05426499,  0.02977670,  0.11404727,  0.09945221],\n",
      "          [-0.06621393, -0.07823639, -0.06912635,  0.05457773, -0.00936087],\n",
      "          [ 0.02024053,  0.06298222,  0.05786319, -0.03381032,  0.04733275]],\n",
      "\n",
      "         [[-0.06593055, -0.07923815,  0.01651249,  0.04688665, -0.04822629],\n",
      "          [-0.02842280,  0.07060724, -0.06389017, -0.15853792, -0.07395336],\n",
      "          [ 0.01006375,  0.07462056, -0.02430158, -0.06790108, -0.03668319],\n",
      "          [ 0.07827827,  0.05198185, -0.06501245,  0.03204183,  0.20982318],\n",
      "          [-0.08638497,  0.06419251, -0.07158568, -0.00529421, -0.02087413]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.03393113, -0.08271125,  0.02041319,  0.04616671,  0.13629910],\n",
      "          [-0.10986833,  0.03826354,  0.05981230, -0.03306085, -0.04996925],\n",
      "          [ 0.03211767, -0.03246081,  0.13310264, -0.04173230, -0.00205524],\n",
      "          [ 0.05793827, -0.02760234, -0.02572171, -0.03984093, -0.04297478],\n",
      "          [-0.06150466, -0.01952822,  0.01396992,  0.05304113,  0.02468647]],\n",
      "\n",
      "         [[ 0.01376449,  0.03938069,  0.01149357,  0.10825098,  0.08834018],\n",
      "          [-0.00368270,  0.05435447,  0.00354751, -0.08157624, -0.03122849],\n",
      "          [ 0.00440023, -0.04778880, -0.05539897, -0.05178137, -0.00790940],\n",
      "          [-0.00549599, -0.06398467,  0.04175593, -0.03340815, -0.07857146],\n",
      "          [ 0.05851837,  0.09970795,  0.03199684,  0.10427931, -0.11310953]],\n",
      "\n",
      "         [[ 0.14792199, -0.06690830,  0.01411365, -0.00058303,  0.12918684],\n",
      "          [-0.03707269,  0.06201976,  0.12908266, -0.02716819,  0.03434102],\n",
      "          [ 0.13299733,  0.04204443,  0.06115631, -0.02596248,  0.05911577],\n",
      "          [ 0.13408585, -0.01330096, -0.05075085, -0.02192910,  0.05789414],\n",
      "          [-0.00074550,  0.00182649,  0.00986567, -0.03074306,  0.01911138]]],\n",
      "\n",
      "\n",
      "        [[[-0.03725875, -0.06140297,  0.03168594,  0.00838635,  0.00938935],\n",
      "          [ 0.10573972, -0.08355468, -0.02017087, -0.03043095, -0.06246676],\n",
      "          [-0.06314994, -0.07474132, -0.02639798, -0.07785455, -0.01051079],\n",
      "          [ 0.05073320,  0.04078016, -0.06238253,  0.08710585,  0.06423744],\n",
      "          [-0.03347686,  0.08680937,  0.04582042, -0.10890278,  0.01824339]],\n",
      "\n",
      "         [[-0.03952609,  0.13392264, -0.06514899, -0.05094520,  0.00478079],\n",
      "          [ 0.05198830,  0.00821417,  0.01266646,  0.02014027,  0.05999838],\n",
      "          [-0.00340272,  0.08939803, -0.00059625,  0.10723002, -0.09372249],\n",
      "          [ 0.02715235,  0.07974571,  0.00712920,  0.02431520,  0.03512901],\n",
      "          [-0.02868374, -0.07645535,  0.07165837, -0.05497815,  0.04677773]],\n",
      "\n",
      "         [[-0.14220729, -0.01642417, -0.04778175,  0.05426632,  0.11182562],\n",
      "          [ 0.01180977,  0.01818160, -0.04990768, -0.01284826, -0.06897558],\n",
      "          [-0.08127917,  0.05318227,  0.06668033,  0.02802232,  0.04250106],\n",
      "          [-0.11069144,  0.09053962,  0.07228538, -0.07820071, -0.05467565],\n",
      "          [-0.12351068,  0.08575661, -0.02173880, -0.01416278,  0.08615170]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.13423714, -0.03815886,  0.03816330, -0.01621404, -0.07715316],\n",
      "          [-0.04702605,  0.00887390,  0.02161213, -0.04889238, -0.03601886],\n",
      "          [-0.02038899, -0.04671028,  0.01882195, -0.12411432,  0.06922542],\n",
      "          [ 0.04930260, -0.07263442,  0.04453302, -0.03010605, -0.01842250],\n",
      "          [-0.00893493,  0.11544286,  0.08021139, -0.03618741,  0.04629109]],\n",
      "\n",
      "         [[ 0.12497535,  0.06645298,  0.07601072,  0.02511500, -0.02711841],\n",
      "          [ 0.00474245, -0.06479405,  0.11967541, -0.10737073,  0.04680381],\n",
      "          [-0.10329045, -0.03309850, -0.04942237,  0.13243797,  0.08414938],\n",
      "          [ 0.05737696,  0.00524121,  0.00740493, -0.10233473,  0.09649734],\n",
      "          [-0.08374887,  0.03654788, -0.03952739,  0.02570211, -0.04423657]],\n",
      "\n",
      "         [[-0.04342224, -0.03979274, -0.03626437,  0.07393731,  0.11807676],\n",
      "          [ 0.06074775, -0.06452077, -0.03056546,  0.03358621, -0.13512947],\n",
      "          [ 0.01353175, -0.00045765, -0.06478564, -0.00222581,  0.08562817],\n",
      "          [ 0.04201965,  0.04973603, -0.04101499,  0.05920982, -0.00440654],\n",
      "          [ 0.06769152,  0.05247637, -0.04876187,  0.04940811,  0.05720046]]],\n",
      "\n",
      "\n",
      "        [[[ 0.07296643, -0.02238671,  0.04880418,  0.05007473,  0.05229439],\n",
      "          [ 0.08153880,  0.03813653,  0.01657565, -0.02329348, -0.07657525],\n",
      "          [-0.03147863, -0.06040520,  0.04819846, -0.02638522,  0.02811828],\n",
      "          [-0.04738356, -0.06207188, -0.05061458, -0.00616325,  0.06493483],\n",
      "          [-0.02208322,  0.02398385,  0.08606802, -0.10161741, -0.00498961]],\n",
      "\n",
      "         [[ 0.05628188,  0.06561081, -0.05283130, -0.02269553,  0.00519735],\n",
      "          [-0.04522982,  0.04156234,  0.06538405,  0.05652884, -0.02073733],\n",
      "          [ 0.02002987,  0.07048202,  0.03125889,  0.02699916, -0.02652232],\n",
      "          [-0.01245643, -0.08906759,  0.03163455,  0.04250871,  0.08008271],\n",
      "          [ 0.03875295,  0.02140429, -0.00159948,  0.00532227, -0.06753109]],\n",
      "\n",
      "         [[ 0.05098309,  0.05519797, -0.11599406,  0.02094416, -0.12674445],\n",
      "          [ 0.04749368, -0.06697400, -0.06215895,  0.06356204,  0.06708340],\n",
      "          [ 0.06114836,  0.05913837, -0.01162001, -0.05735631,  0.02178494],\n",
      "          [ 0.06034366, -0.02576882, -0.09828721, -0.01903550,  0.05332101],\n",
      "          [-0.00135989, -0.00547090,  0.04619619,  0.06791047, -0.02876070]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.09237070, -0.06885990,  0.00843302,  0.02319575, -0.09126038],\n",
      "          [-0.20622770,  0.05770314, -0.05524185,  0.06308909,  0.01949865],\n",
      "          [ 0.05947011,  0.00093663,  0.02957422,  0.04034602,  0.00758246],\n",
      "          [ 0.03853898,  0.01247289,  0.06215080,  0.02034242, -0.03157216],\n",
      "          [-0.12914890, -0.06377766,  0.01450350, -0.05276486,  0.02220368]],\n",
      "\n",
      "         [[ 0.06516230, -0.05895260, -0.06443317, -0.05874763,  0.02194979],\n",
      "          [-0.07734998,  0.05464603,  0.04575077,  0.03758018,  0.14904568],\n",
      "          [ 0.07737642, -0.17081270,  0.11026025,  0.00356938,  0.04196765],\n",
      "          [-0.08771687,  0.01075565,  0.03652329,  0.09294744,  0.08286404],\n",
      "          [-0.07271910, -0.01015406,  0.11786053, -0.03865341, -0.00011255]],\n",
      "\n",
      "         [[-0.04666478, -0.06141014, -0.02815867,  0.02166881, -0.00056359],\n",
      "          [ 0.07982340, -0.04866859,  0.07978345,  0.03913150,  0.15458646],\n",
      "          [ 0.02300316, -0.04921987, -0.00396419, -0.04221912,  0.00930112],\n",
      "          [-0.01933390, -0.12113122,  0.00977334,  0.00586604, -0.09546758],\n",
      "          [-0.11679827,  0.01662397,  0.01081836,  0.00616634, -0.08532871]]]])\n",
      "conv2.bias Parameter containing:\n",
      "Tensor(shape=[20], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n",
      "fc.weight Parameter containing:\n",
      "Tensor(shape=[980, 10], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [[-0.04594205, -0.03668004,  0.03592646, ...,  0.05482418,\n",
      "         -0.01147358, -0.01687970],\n",
      "        [-0.02310931, -0.07620153, -0.07206465, ...,  0.04278419,\n",
      "          0.03364198, -0.01683033],\n",
      "        [ 0.02447041,  0.07251372,  0.02300600, ..., -0.06937405,\n",
      "          0.03590800,  0.06482572],\n",
      "        ...,\n",
      "        [ 0.05429860, -0.00249490, -0.07503617, ..., -0.05303785,\n",
      "         -0.01864232,  0.06126241],\n",
      "        [-0.00797569,  0.05125344, -0.01472158, ...,  0.06139667,\n",
      "         -0.06696133,  0.03713059],\n",
      "        [ 0.07011503,  0.07623947,  0.03546768, ...,  0.05477322,\n",
      "          0.02028764, -0.04694789]])\n",
      "fc.bias Parameter containing:\n",
      "Tensor(shape=[10], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc.weight', 'fc.bias'])\n",
      "conv1.weight: Parameter containing:\n",
      "Tensor(shape=[20, 1, 5, 5], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [[[[ 0.00685916, -0.02827659, -0.00629800,  0.17840426,  0.07073421],\n",
      "          [ 0.15301149,  0.14512494, -0.08644208, -0.04089865,  0.06347622],\n",
      "          [-0.17223580,  0.53609502, -0.31562763,  0.31539580,  0.11763398],\n",
      "          [ 0.06057844,  0.12100003, -0.19036214, -0.32686502, -0.04253779],\n",
      "          [-0.24231870, -0.09162699, -0.00356645,  0.13624071,  0.35424659]]],\n",
      "\n",
      "\n",
      "        [[[ 0.19513041, -0.18949197, -0.06306379, -0.33953783, -0.12144631],\n",
      "          [-0.52912796, -0.04904719,  0.22196575, -0.21367665, -0.14066246],\n",
      "          [-0.47838733,  0.19762798,  0.32846957, -0.06876396, -0.05648636],\n",
      "          [-0.49627945,  0.04398900, -0.02770269, -0.09062629, -0.25601473],\n",
      "          [ 0.22629717,  0.12104782, -0.21879482, -0.29909152,  0.15422553]]],\n",
      "\n",
      "\n",
      "        [[[-0.57517582,  0.07872081,  0.24480270, -0.10398132, -0.21764469],\n",
      "          [-0.15390907,  0.03888793,  0.31359494,  0.05783065,  0.09863662],\n",
      "          [-0.00036609,  0.05385838, -0.06690086,  0.12735263, -0.56700391],\n",
      "          [-0.39829355, -0.05769483, -0.05732801,  0.33780238,  0.53295583],\n",
      "          [-0.15284447, -0.32235619, -0.12371407, -0.02103593,  0.06312265]]],\n",
      "\n",
      "\n",
      "        [[[ 0.23172377,  0.32362208,  0.05168466,  0.50253624, -0.42746297],\n",
      "          [ 0.18174489, -0.13101643, -0.20413736,  0.17553115,  0.19926409],\n",
      "          [ 0.52452153, -0.21518429, -0.14862841, -0.26547405,  0.15221493],\n",
      "          [ 0.24805403,  0.24819772, -0.02215577,  0.04740265,  0.31030568],\n",
      "          [ 0.13231085,  0.28731391,  0.17576139,  0.13701966, -0.38116997]]],\n",
      "\n",
      "\n",
      "        [[[ 0.12775137,  0.03297117, -0.17570035,  0.18195666, -0.47419620],\n",
      "          [ 0.33481011, -0.00369794,  0.17601000,  0.09904220, -0.01528732],\n",
      "          [-0.27129415, -0.38718089,  0.54803246,  0.14578563,  0.23014370],\n",
      "          [ 0.22600482, -0.23167014,  0.17863478, -0.02830615, -0.00889434],\n",
      "          [-0.53870600,  0.09300359, -0.09527846, -0.37450442,  0.08891475]]],\n",
      "\n",
      "\n",
      "        [[[-0.63375968,  0.01352617, -0.40009904, -0.61710560,  0.42513108],\n",
      "          [-0.17352255, -0.15771994,  0.08855379,  0.08413874,  0.15258929],\n",
      "          [ 0.29868862,  0.07071455, -0.06578474,  0.07363825, -0.37690639],\n",
      "          [ 0.44038859,  0.05002983, -0.36027536, -0.08144029, -0.13867600],\n",
      "          [ 0.20790854, -0.23763619,  0.25140327, -0.20502961,  0.51529437]]],\n",
      "\n",
      "\n",
      "        [[[-0.21660955,  0.22310224, -0.69941777,  0.03165551, -0.17004766],\n",
      "          [-0.53551018, -0.00880008, -0.03999506, -0.01059055,  0.07984187],\n",
      "          [-0.24571578,  0.48378825,  0.23929246, -0.21449521, -0.02937231],\n",
      "          [-0.16947500,  0.06570072,  0.26996130, -0.00225378, -0.45664248],\n",
      "          [ 0.07661092, -0.02118895, -0.23899272, -0.46309194,  0.00255588]]],\n",
      "\n",
      "\n",
      "        [[[ 0.01068391,  0.18249844, -0.57028669, -0.02930197, -0.49979338],\n",
      "          [ 0.23231256,  0.00567782, -0.46816435, -0.05387615, -0.06768128],\n",
      "          [-0.57407957, -0.38469478,  0.14671670, -0.19064607,  0.16242966],\n",
      "          [ 0.05997788,  0.14379767, -0.43261734, -0.17703882,  0.15757072],\n",
      "          [ 0.13257961, -0.19664960, -0.09354297,  0.29407963, -0.09218543]]],\n",
      "\n",
      "\n",
      "        [[[-0.05021130,  0.17844844, -0.42766276, -0.07062170,  0.16383797],\n",
      "          [ 0.21554719, -0.24867499, -0.04475411, -0.39096946,  0.26923472],\n",
      "          [-0.38155058, -0.20673595,  0.26717725,  0.17506047, -0.13850974],\n",
      "          [ 0.24715245,  0.50960469, -0.26948437, -0.34616211,  0.73909211],\n",
      "          [-0.04646778,  0.39078084, -0.20954891,  0.18042360, -0.52201396]]],\n",
      "\n",
      "\n",
      "        [[[ 0.08273765, -0.09105711, -0.12958071, -0.29861581, -0.19989121],\n",
      "          [ 0.18611236, -0.67686284, -0.18584950, -0.06461700, -0.56305975],\n",
      "          [ 0.06481798, -0.14299046, -0.17562066, -0.22412276, -0.28058848],\n",
      "          [ 0.37638661, -0.01545838,  0.02834374, -0.44421050,  0.24978781],\n",
      "          [ 0.47179404,  0.16267806,  0.40233117,  0.03811143, -0.01092976]]],\n",
      "\n",
      "\n",
      "        [[[ 0.02333970,  0.22006829, -0.54471511, -0.18041711, -0.12503794],\n",
      "          [ 0.10296388,  0.66934699,  0.06700514, -0.20929156, -0.44396263],\n",
      "          [ 0.17440335, -0.11310478, -0.27263820, -0.03610590,  0.34677291],\n",
      "          [ 0.10419460, -0.10790036, -0.02151075, -0.10813204,  0.24426167],\n",
      "          [-0.68452549, -0.05186544,  0.42406222,  0.13648765,  0.30811521]]],\n",
      "\n",
      "\n",
      "        [[[ 0.09500170, -0.36259845, -0.01421565, -0.51392865,  0.03173811],\n",
      "          [ 0.30929992, -0.37484398, -0.67092603,  0.38448986,  0.20572491],\n",
      "          [-0.06885525, -0.08177355,  0.03418944,  0.09427575, -0.09787619],\n",
      "          [-0.19711410,  0.03484981, -0.18318318, -0.29350778, -0.12663999],\n",
      "          [ 0.28633794,  0.76515442, -0.18395521,  0.04850828, -0.42525050]]],\n",
      "\n",
      "\n",
      "        [[[-0.35616848,  0.21384183, -0.30207559, -0.24183397, -0.18685399],\n",
      "          [-0.10017283, -0.26885203, -0.00999357, -0.20930922, -0.30007312],\n",
      "          [ 0.04770488, -0.29103780,  0.41294131, -0.16971968,  0.12665810],\n",
      "          [-0.17194323,  0.01729790, -0.00445249, -0.09733340, -0.43409133],\n",
      "          [-0.20486255,  0.16031756, -0.08515637, -0.48493597, -0.27581355]]],\n",
      "\n",
      "\n",
      "        [[[-0.12680070,  0.45840842,  0.34614697,  0.01164691, -0.18574916],\n",
      "          [ 0.38707557, -0.22326174, -0.49643895, -0.20572264,  0.16056405],\n",
      "          [ 0.12451726, -0.24313389,  0.52626842, -0.06479061,  0.00620653],\n",
      "          [-0.50085860,  0.08915006, -0.18392627, -0.27886039, -0.44590169],\n",
      "          [-0.05520649, -0.00192035,  0.19382079,  0.30146438, -0.19075648]]],\n",
      "\n",
      "\n",
      "        [[[ 0.17200373,  0.36045110,  0.41303283, -0.05697197,  0.09817324],\n",
      "          [ 0.24160154,  0.20104299, -0.18491277,  0.30328605, -0.15703280],\n",
      "          [ 0.14609288, -0.35724214,  0.21076484, -0.39886394, -0.52864009],\n",
      "          [-0.22453904, -0.02393566,  0.22230516, -0.23590966,  0.08467520],\n",
      "          [ 0.10224124,  0.40317741, -0.02853043, -0.15255164, -0.10038172]]],\n",
      "\n",
      "\n",
      "        [[[ 0.25365898, -0.12408470,  0.03224292, -0.62302363, -0.00868563],\n",
      "          [-0.20751679, -0.27265713,  0.10152783, -0.07019686,  0.20044255],\n",
      "          [-0.01536619,  0.24629453, -0.50585532,  0.23426042,  0.34668776],\n",
      "          [ 0.37670568, -0.04776695,  0.13384424, -0.14209406, -0.57875556],\n",
      "          [-0.26221901, -0.30681297, -0.01673537,  0.17959179,  0.21828853]]],\n",
      "\n",
      "\n",
      "        [[[-0.16501173,  0.17561746,  0.07628912, -0.11273076,  0.13463901],\n",
      "          [ 0.11364105,  0.01849935, -0.31740916, -0.60119075,  0.04458283],\n",
      "          [-0.07207803,  0.12399440,  0.31883177, -0.04306261,  0.31809899],\n",
      "          [ 0.53678864,  0.46501875,  0.10381963, -0.00760704,  0.09934480],\n",
      "          [ 0.29437843, -0.22324999,  0.09065028, -0.09417098, -0.08439593]]],\n",
      "\n",
      "\n",
      "        [[[ 0.03022816,  0.12597114,  0.43907633,  0.34548521, -0.25947937],\n",
      "          [-0.45549130,  0.52134931, -0.32358417,  0.20811863, -0.34024143],\n",
      "          [-0.43153605, -0.25069806, -0.29741403, -0.22896214, -0.38898811],\n",
      "          [ 0.54590076, -0.12959005,  0.05260053, -0.17379124, -0.30745879],\n",
      "          [-0.37525591, -0.09658249, -0.17254332,  0.05943358, -0.03464231]]],\n",
      "\n",
      "\n",
      "        [[[ 0.20201388,  0.02919013, -0.08088612,  0.01559477,  0.17472650],\n",
      "          [ 0.02217738,  0.36532468,  0.09082818,  0.32516485,  0.62717056],\n",
      "          [-0.41841173, -0.20961802, -0.27907607, -0.53327650, -0.65651703],\n",
      "          [-0.03331180,  0.14785196,  0.18553858,  0.01868903, -0.59221572],\n",
      "          [ 0.06131181, -0.03520117, -0.85177898,  0.18555664,  0.01231959]]],\n",
      "\n",
      "\n",
      "        [[[-0.62316769,  0.08701386,  0.14838187, -0.23780005,  0.56156313],\n",
      "          [-0.15943550,  0.24376908,  0.34804633, -0.05670764, -0.17174038],\n",
      "          [-0.68133259, -0.22665225,  0.22788525,  0.81866211, -0.13602504],\n",
      "          [-0.25465700, -0.15599780, -0.28056905,  0.16332000,  0.44518515],\n",
      "          [-0.02113093, -0.30859464, -0.10784610,  0.37195674,  0.19204357]]]])\n"
     ]
    }
   ],
   "source": [
    "model = MNIST()\n",
    "\n",
    "# 遍历所有的参数名和参数值\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param)  \n",
    "    \n",
    "state_dict = model.state_dict() #collections.OrderedDict类型\n",
    "\n",
    "print(state_dict.keys())\n",
    "\n",
    "# 根据某个参数的名字，打印对应的参数值\n",
    "# 如打印“conv1.weight”的参数值\n",
    "print(\"conv1.weight:\",state_dict[\"conv1.weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "通过`print(state_dict.keys())`打印的结果为`['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc.weight', 'fc.bias']`，对应我们模型结构：两个卷积层，一个全连接层。每一层都有一个权重项和偏置项。由于有两个卷积层，所以卷积的权重名称有对应的编号。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[paddle.ParamAttr](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/ParamAttr_cn.html#cn-api-fluid-paramattr)可创建一个参数属性的对象，用户可设置参数的名称`name`、初始化方式`initializer`、学习率`learning_rate`、正则化规则`regularizer`、是否需要训练`trainable`、梯度裁剪方式`need_clip`、是否做模型平均`do_model_average`等属性。其参数初始化方式`initializer`默认值为`None`，表示权重参数采用Xavier初始化方式，偏置参数采用全0初始化方式。Xavier初始化方式可以缓解梯度消失的问题。\n",
    "\n",
    "下面简单介绍几个常用的权重初始化方法：\n",
    "* [XavierUniform](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/initializer/XavierUniform_cn.html#xavieruniform)、[XavierNormal](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/initializer/XavierNormal_cn.html)初始化函数用于保持所有层的梯度尺度几乎一致。\n",
    "* [Constant](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/initializer/Constant_cn.html)为常量初始化函数，用于权重初始化，通过输入的value值初始化输入变量。\n",
    "* [KaimingNormal](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/initializer/KaimingNormal_cn.html)、[KaimingUniform](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/initializer/KaimingUniform_cn.html)方法来自Kaiming He，Xiangyu Zhang，Shaoqing Ren 和 Jian Sun所写的论文: [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)。这是一个鲁棒性特别强的初始化方法，并且适应了非线性激活函数（rectifier nonlinearities）。\n",
    "* [Normal](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/initializer/Normal_cn.html)随机正态（高斯）分布初始化函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "下面我们通过`paddle.ParamAttr`基于常量初始化函数`Constant`完成全连接层的参数初始化，具体如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', Parameter containing:\n",
      "Tensor(shape=[20, 10], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [[3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]])), ('bias', Parameter containing:\n",
      "Tensor(shape=[10], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))])\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "from paddle.nn import Linear\n",
    "\n",
    "weight_attr = paddle.ParamAttr(initializer=paddle.nn.initializer.Constant(3))\n",
    "#weight_attr = paddle.ParamAttr(initializer=paddle.nn.initializer.XavierNormal())\n",
    "\n",
    "fc = Linear(in_features=20, out_features=10,weight_attr=weight_attr)\n",
    "\n",
    "print(fc.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**在我们的实际应用中，由于训练数据有限、数据获取较难、训练资源有限等原因，往往利用在大规模开源数据集上训练得到的模型参数作为我们自己模型的初始值（也称为预训练模型），这样可以加速网络训练、并得到较高精度。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 作业 2-3\n",
    "\n",
    "在手写数字识别任务上，哪种优化算法的效果最好？多大的学习率最优？（可通过Loss的下降趋势来判断）\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
