{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.9 手写数字识别之恢复训练\n",
    "\n",
    "在上一节中，我们已经介绍了将训练好的模型保存到磁盘文件的方法。应用程序可以随时加载模型，完成预测任务。但是在日常训练工作中我们会遇到一些突发情况，导致训练过程主动或被动的中断。如果训练一个模型需要花费几天的训练时间，中断后从初始状态重新训练是不可接受的。万幸的是，飞桨支持从上一次保存状态开始训练，只要我们随时保存训练过程中的模型状态，就不用从初始状态重新训练。\n",
    "\n",
    "## 2.9.1 构建并训练网络\n",
    "\n",
    "下面介绍恢复训练的实现方法，依然使用手写数字识别的案例，网络定义的部分保持不变，直接调用封装好的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import os\n",
    "from data_process import get_MNIST_dataloader\n",
    "from MNIST_network import MNIST\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "train_loader, test_loader = get_MNIST_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义训练Trainer，包含训练过程和模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model_path, model, optimizer):\n",
    "        self.model_path = model_path   # 模型存放路径 \n",
    "        self.model = model             # 定义的模型\n",
    "        self.optimizer = optimizer     # 优化器\n",
    "\n",
    "    def save(self):\n",
    "        # 保存模型\n",
    "        paddle.save(self.model.state_dict(), self.model_path)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        images, labels = data\n",
    "        # 前向计算的过程\n",
    "        predicts = self.model(images)\n",
    "        # 计算损失\n",
    "        loss = F.cross_entropy(predicts, labels)\n",
    "        avg_loss = paddle.mean(loss)\n",
    "        # 后向传播，更新参数的过程\n",
    "        avg_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.clear_grad()\n",
    "        return avg_loss\n",
    "\n",
    "    def train_epoch(self, datasets, epoch):\n",
    "        self.model.train()\n",
    "        for batch_id, data in enumerate(datasets()):\n",
    "            loss = self.train_step(data)\n",
    "            # 每训练了1000批次的数据，打印下当前Loss的情况\n",
    "            if batch_id % 500 == 0:\n",
    "                print(\"epoch_id: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, loss.numpy()))\n",
    "\n",
    "    def train(self, train_datasets, start_epoch, end_epoch, save_path):\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        for i in range(start_epoch, end_epoch):\n",
    "            self.train_epoch(train_datasets, i)\n",
    "\n",
    "            paddle.save(self.optimizer.state_dict(), './{}/mnist_epoch{}'.format(save_path,i)+'.pdopt')\n",
    "            paddle.save(self.model.state_dict(), './{}/mnist_epoch{}'.format(save_path,i)+'.pdparams')\n",
    "\n",
    "        self.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在开始介绍使用飞桨恢复训练前，先正常训练一个模型，优化器使用Adam，使用动态变化的学习率，学习率从0.01衰减到0.001。每训练一轮后保存一次模型，之后将采用其中某一轮的模型参数进行恢复训练，验证一次性训练和中断再恢复训练的模型表现是否相差不多（训练loss的变化）。\n",
    "\n",
    "注意进行恢复训练的程序不仅要保存模型参数，还要保存优化器参数。这是因为某些优化器含有一些随着训练过程变换的参数，例如Adam、Adagrad等优化器采用可变学习率的策略，随着训练进行会逐渐减少学习率。这些优化器的参数对于恢复训练至关重要。\n",
    "\n",
    "为了演示这个特性，训练程序使用Adam优化器，PolynomialDecay的变化策略将学习率从0.01衰减到0.001。\n",
    "\n",
    "> *class* paddle.optimizer.lr.PolynomialDecay *(learningrate, decaysteps, endlr=0.0001, power=1.0, cycle=False, lastepoch=- 1, verbose=False)*\n",
    "\n",
    "参数说明如下：\n",
    "\n",
    "* learning_rate (float)：初始学习率，数据类型为Python float。\n",
    "* decay_steps (int)：进行衰减的步长，这个决定了衰减周期。\n",
    "* end_lr (float，可选）：最小的最终学习率，默认值为0.0001。\n",
    "* power (float，可选)：多项式的幂，默认值为1.0。\n",
    "* last_epoch (int，可选)：上一轮的轮数，重启训练时设置为上一轮的epoch数。默认值为-1，则为初始学习率。\n",
    "* verbose (bool，可选)：如果是 True，则在每一轮更新时在标准输出stdout输出一条信息，默认值为False。\n",
    "* cycle (bool，可选)：学习率下降后是否重新上升。若为True，则学习率衰减到最低学习率值时会重新上升。若为False，则学习率单调递减。默认值为False。\n",
    "\n",
    "PolynomialDecay的变化曲线下图所示：\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/4877393d22a445098adc002b53ebbbcef82f8b0ac46d409ca7e3a19e2622fe8a\" width=300> \n",
    "<br></br>\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0901 17:29:43.145830   192 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0901 17:29:43.152843   192 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_id: 0, batch_id: 0, loss is: [3.900196]\n",
      "epoch_id: 0, batch_id: 500, loss is: [0.1180672]\n",
      "epoch_id: 1, batch_id: 0, loss is: [0.05891193]\n",
      "epoch_id: 1, batch_id: 500, loss is: [0.16080402]\n",
      "epoch_id: 2, batch_id: 0, loss is: [0.05444674]\n",
      "epoch_id: 2, batch_id: 500, loss is: [0.02639692]\n"
     ]
    }
   ],
   "source": [
    "#在使用GPU机器时，可以将use_gpu变量设置成True\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')\n",
    "\n",
    "paddle.seed(1024)\n",
    "\n",
    "epochs = 3\n",
    "BATCH_SIZE = 32\n",
    "model_path = './mnist.pdparams'\n",
    "\n",
    "model = MNIST()\n",
    "\n",
    "total_steps = (int(50000//BATCH_SIZE) + 1) * epochs\n",
    "lr = paddle.optimizer.lr.PolynomialDecay(learning_rate=0.01, decay_steps=total_steps, end_lr=0.001)\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters())\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_path=model_path,\n",
    "    model=model,\n",
    "    optimizer=optimizer\n",
    ")\n",
    "\n",
    "trainer.train(train_datasets=train_loader, start_epoch = 0, end_epoch = epochs, save_path='checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9.2 恢复训练\n",
    "\n",
    "> 模型恢复训练，需要重新组网，所以我们需要重启AI Studio，运行数据处理和`MNIST`网络定义、`Trainer`部分代码，再执行模型恢复代码。\n",
    "> \n",
    "\n",
    "\n",
    "在上述训练代码中，我们训练了3轮（epoch）。在每轮结束时，我们均保存了模型参数和优化器相关的参数。\n",
    "\n",
    "- 使用``model.state_dict()``获取模型参数。\n",
    "- 使用``opt.state_dict``获取优化器和学习率相关的参数。\n",
    "- 调用``paddle.save``将参数保存到本地。\n",
    "\n",
    "比如第一轮训练保存的文件是mnist_epoch0.pdparams，mnist_epoch0.pdopt，分别存储了模型参数和优化器参数。\n",
    "\n",
    "使用``paddle.load``分别加载模型参数和优化器参数，如下代码所示。\n",
    "\n",
    "``` \n",
    "paddle.load(params_path+'.pdparams')\n",
    "paddle.load(params_path+'.pdopt')\n",
    "``` \n",
    "\n",
    "如何判断模型是否准确的恢复训练呢？\n",
    "\n",
    "理想的恢复训练是模型状态回到训练中断的时刻，恢复训练之后的梯度更新走向是和恢复训练前的梯度走向完全相同的。基于此，我们可以通过恢复训练后的损失变化，判断上述方法是否能准确的恢复训练。即从epoch 0结束时保存的模型参数和优化器状态恢复训练，校验其后训练的损失变化（epoch 1）是否和不中断时的训练相差不多。\n",
    "\n",
    "------\n",
    "**说明：**\n",
    "\n",
    "恢复训练有如下两个要点：\n",
    "* 保存模型时分别保存模型参数和优化器参数。\n",
    "* 恢复参数时分别恢复模型参数和优化器参数。\n",
    "\n",
    "------\n",
    "\n",
    "下面的代码将展示恢复训练的过程，并验证恢复训练是否成功。加载模型参数并从第一个epoch开始训练，以便读者可以校验恢复训练后的损失变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0901 17:31:41.696164   509 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0901 17:31:41.700848   509 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_id: 1, batch_id: 0, loss is: [0.03602091]\n",
      "epoch_id: 1, batch_id: 500, loss is: [0.4263561]\n",
      "epoch_id: 2, batch_id: 0, loss is: [0.0733113]\n",
      "epoch_id: 2, batch_id: 500, loss is: [0.13029066]\n"
     ]
    }
   ],
   "source": [
    "# MLP继续训练\n",
    "paddle.seed(1024)\n",
    "\n",
    "epochs = 3\n",
    "BATCH_SIZE = 32\n",
    "model_path = './mnist_retrain.pdparams'\n",
    "\n",
    "model = MNIST()\n",
    "# lr = 0.01\n",
    "total_steps = (int(50000//BATCH_SIZE) + 1) * epochs\n",
    "lr = paddle.optimizer.lr.PolynomialDecay(learning_rate=0.01, decay_steps=total_steps, end_lr=0.001)\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters())\n",
    "\n",
    "params_dict = paddle.load('./checkpoint/mnist_epoch0.pdparams')\n",
    "opt_dict = paddle.load('./checkpoint/mnist_epoch0.pdopt')\n",
    "\n",
    "# 加载参数到模型\n",
    "model.set_state_dict(params_dict)\n",
    "optimizer.set_state_dict(opt_dict)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_path=model_path,\n",
    "    model=model,\n",
    "    optimizer=optimizer\n",
    ")\n",
    "# 前面训练模型都保存了，这里save_path设置为新路径，实际训练中保存在同一目录就可以\n",
    "trainer.train(train_datasets=train_loader,start_epoch = 1, end_epoch = epochs, save_path='checkpoint_con')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以将模型参数和优化器参数分别保存，也可以同时保存，具体见下面的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle import nn\n",
    "from paddle.optimizer import Adam\n",
    "\n",
    "layer = paddle.nn.Linear(3, 4)\n",
    "adam = Adam(learning_rate=0.001, parameters=layer.parameters())\n",
    "obj = {'model': layer.state_dict(), 'opt': adam.state_dict(), 'epoch': 100}\n",
    "path = 'example/model.pdparams'\n",
    "paddle.save(obj, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从恢复训练的损失变化来看，加载模型参数继续训练的损失函数值和正常训练损失函数值是一致，可见使用飞桨实现恢复训练是极其简单的。\n",
    "总结一下：\n",
    "* 保存模型时同时保存模型参数和优化器参数；\n",
    "```\n",
    "paddle.save(opt.state_dict(), 'model.pdopt')\n",
    "paddle.save(model.state_dict(), 'model.pdparams')\n",
    "```\n",
    "* 恢复参数时同时恢复模型参数和优化器参数。\n",
    "```\n",
    "model_dict = paddle.load(\"model.pdparams\")\n",
    "opt_dict = paddle.load(\"model.pdopt\")\n",
    "\n",
    "model.set_state_dict(model_dict)\n",
    "opt.set_state_dict(opt_dict)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
